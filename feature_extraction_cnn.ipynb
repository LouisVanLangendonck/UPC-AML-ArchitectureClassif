{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMl56kOKW/RdvkjjUwEG8OX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LouisVanLangendonck/UPC-AML-ArchitectureClassif/blob/main/feature_extraction_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extractor for Architecture Image Classification**\n"
      ],
      "metadata": {
        "id": "TXr12Gzdmm2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WDhs4F-_dGi",
        "outputId": "62927abc-3626-43c1-bfe5-68b8e73fc91b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.9.2\n",
            "Hub version: 0.12.0\n",
            "keras version: 2.9.0\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "#@title Imports...\n",
        "\n",
        "import itertools\n",
        "import os\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "import random\n",
        "import pickle\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"keras version:\", keras.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to your google drive. Make sure all data (scraped using the webscraper) is loaded in here."
      ],
      "metadata": {
        "id": "6a1j96iTm4wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJEYy0glANJS",
        "outputId": "2eae0d21-b7ef-405c-f814-0101506e4050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_aml_file = '/content/drive/MyDrive/FIB-2022-2023/aml'"
      ],
      "metadata": {
        "id": "2bPtZfAuVfla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifiy where the train and test data is stored. The file-structure should be as follows (which is automatically obtained if data_scraper.ipynb and train_test_split.ipynb are correctly used): \n",
        "- Both train- and test in seperate files. \n",
        "- In each of these, each style should have its own folder containing all images in .jpg format."
      ],
      "metadata": {
        "id": "Tv1HLEmEm2xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = os.path.join(path_to_aml_file,'data/unzipped/train')\n",
        "test_data = os.path.join(path_to_aml_file,'data/unzipped/test')\n",
        "print(os.listdir(train_data))"
      ],
      "metadata": {
        "id": "KC7PtS5VAVgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1029d693-3f7a-4eaa-da95-cd1ff02c0c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gothic', 'baroque', 'modernism', 'contemporary', 'noucentisme', 'renaissance', 'romanesque', 'neoclassicism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Different pre-trained architectures to be used for feature extraction.\n",
        "\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "vgg_model = VGG19(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(299, 299, 3), \n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "\n",
        "Xception_model = Xception(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    input_shape=(299, 299, 3)\n",
        ")\n",
        "\n",
        "InceptionResNet_model = InceptionResNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    input_shape=(299, 299, 3)\n",
        ")\n",
        "\n",
        "EfficientNetB7_model = EfficientNetB7(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    input_shape=(299, 299, 3)\n",
        ")"
      ],
      "metadata": {
        "id": "9v3pSelUltsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ab65c1-2487-4518-ee63-0688fdc82fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219055592/219055592 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258076736/258076736 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specify here which of the previous pre-trained models you want to use for feature extraction** + whether or not to shuffle the data. If not, this would mainly be to later concatenate features (as the order is kept)"
      ],
      "metadata": {
        "id": "t-t7AK53nuVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = Xception_model\n",
        "\n",
        "shuffle_datagen = False #Preferable false as you can concatenate vectors"
      ],
      "metadata": {
        "id": "_obGTJ0KBmHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining flow from directory...\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "batch_size = 150\n",
        "image_height, image_width = feature_extractor.input_shape[1:3]\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        train_data, \n",
        "        target_size = (image_height,image_width),\n",
        "        batch_size=batch_size, \n",
        "        class_mode = 'categorical', \n",
        "        shuffle=shuffle_datagen)\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "        test_data, \n",
        "        target_size = (image_height,image_width),\n",
        "        batch_size=batch_size, \n",
        "        class_mode = 'categorical', shuffle=shuffle_datagen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jnoirs9Bjjk",
        "outputId": "dd2777c2-8cbf-4319-9faf-42323ee61acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12291 images belonging to 8 classes.\n",
            "Found 3135 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nr_train_images = train_generator.samples\n",
        "nr_test_images = test_generator.samples\n",
        "nr_of_target_classes = test_generator.num_classes"
      ],
      "metadata": {
        "id": "Zj71WyH6mVTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_encoding = train_generator.class_indices #Save encoding of classes to use in other files (for concat. or knowing what prediction means)\n",
        "with open(os.path.join(path_to_aml_file,'models/extracted_features/class_encoding.pkl'), 'wb') as f:\n",
        "    pickle.dump(class_encoding, f)"
      ],
      "metadata": {
        "id": "Anryqz7zSdIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Feature extraction loop !\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "#pip install tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features(generator, sample_count):\n",
        "    print('Beginning feature extraction for {} samples in {} batches:'.format(sample_count, int(np.ceil(sample_count/batch_size))))\n",
        "    with tqdm(total=int(np.ceil(sample_count/batch_size)), position=0, leave=True) as pbar:\n",
        "        input_list = [sample_count]\n",
        "        input_list.extend(feature_extractor.output_shape[1:])\n",
        "        features = np.zeros(shape = tuple(input_list))\n",
        "        labels = np.zeros(shape = (sample_count, nr_of_target_classes))\n",
        "        i = 0\n",
        "        for inputs_batch, labels_batch in generator:\n",
        "            pbar.update(n=1)\n",
        "            features_batch = feature_extractor.predict(inputs_batch, verbose=0)\n",
        "            features[i*batch_size:(i+1)*batch_size] = features_batch\n",
        "            labels[i*batch_size : (i+1)*batch_size] = labels_batch\n",
        "            i += 1\n",
        "            if (i+1)*batch_size >= sample_count:\n",
        "                print('final batch')\n",
        "                features_batch = feature_extractor.predict(inputs_batch, verbose=0)\n",
        "                features[i*batch_size:sample_count] = features_batch[0:sample_count-(i*batch_size)]\n",
        "                labels[i*batch_size:sample_count] = labels_batch[0:sample_count-(i*batch_size)]\n",
        "                break\n",
        "    print('Features extracted!')\n",
        "    print('Shape of feature vector:{}'.format(features.shape))\n",
        "    print('Shape of labels vector:{}'.format(labels.shape))\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "2Gjwl0k6AXNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Feature Extraction:')\n",
        "train_features, train_labels = extract_features(train_generator, nr_train_images)\n",
        "print('Test Feature Extraction:')\n",
        "test_features, test_labels = extract_features(test_generator, nr_test_images)"
      ],
      "metadata": {
        "id": "uYNpGbimS-kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save features in vectors in specified location\n",
        "\n",
        "all_features = np.asarray([(train_features, train_labels), (test_features, test_labels)], dtype=object)\n",
        "np.save(os.path.join(path_to_aml_file,'models/Xception_model_avg_features_CONCAT.npy'), all_features)"
      ],
      "metadata": {
        "id": "tUslbIeJFDPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title If you want to run all extractors at the same time!\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "#pip install tqdm\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "\n",
        "models = [vgg_model, Xception_model, InceptionResNet_model, EfficientNetB7_model]\n",
        "\n",
        "for extr_model in models:\n",
        "  datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "  batch_size = 150\n",
        "  image_height, image_width = feature_extractor.input_shape[1:3]\n",
        "\n",
        "  train_generator = datagen.flow_from_directory(\n",
        "          train_data, \n",
        "          target_size = (image_height,image_width),\n",
        "          batch_size=batch_size, \n",
        "          class_mode = 'categorical', \n",
        "          shuffle=shuffle_datagen)\n",
        "\n",
        "  test_generator = datagen.flow_from_directory(\n",
        "          test_data, \n",
        "          target_size = (image_height,image_width),\n",
        "          batch_size=batch_size, \n",
        "          class_mode = 'categorical', shuffle=shuffle_datagen)\n",
        "  \n",
        "  nr_train_images = train_generator.samples\n",
        "  nr_test_images = test_generator.samples\n",
        "  nr_of_target_classes = test_generator.num_classes\n",
        "  feature_extractor = extr_model\n",
        "  print('Train Feature Extraction:')\n",
        "  train_features, train_labels = extract_features(train_generator, nr_train_images)\n",
        "  print('Test Feature Extraction:')\n",
        "  test_features, test_labels = extract_features(test_generator, nr_test_images)\n",
        "  all_features = np.asarray([(train_features, train_labels), (test_features, test_labels)], dtype=object)\n",
        "  np.save(os.path.join(path_to_aml_file,'models/extracted_features/{}.npy'.format(extr_model.name)), all_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNgke4na5lj2",
        "outputId": "dc8216b0-169e-46ba-a67f-98347634ff3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12291 images belonging to 8 classes.\n",
            "Found 3135 images belonging to 8 classes.\n",
            "Train Feature Extraction:\n",
            "Beginning feature extraction for 12291 samples in 82 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 81/82 [1:08:34<00:46, 46.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 81/82 [1:08:38<00:50, 50.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(12291, 512)\n",
            "Shape of labels vector:(12291, 8)\n",
            "Test Feature Extraction:\n",
            "Beginning feature extraction for 3135 samples in 21 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 20/21 [16:26<00:46, 46.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 20/21 [16:30<00:49, 49.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(3135, 512)\n",
            "Shape of labels vector:(3135, 8)\n",
            "Found 12291 images belonging to 8 classes.\n",
            "Found 3135 images belonging to 8 classes.\n",
            "Train Feature Extraction:\n",
            "Beginning feature extraction for 12291 samples in 82 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 81/82 [10:31<00:07,  7.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 81/82 [10:34<00:07,  7.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(12291, 2048)\n",
            "Shape of labels vector:(12291, 8)\n",
            "Test Feature Extraction:\n",
            "Beginning feature extraction for 3135 samples in 21 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 20/21 [02:48<00:08,  8.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 20/21 [02:51<00:08,  8.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(3135, 2048)\n",
            "Shape of labels vector:(3135, 8)\n",
            "Found 12291 images belonging to 8 classes.\n",
            "Found 3135 images belonging to 8 classes.\n",
            "Train Feature Extraction:\n",
            "Beginning feature extraction for 12291 samples in 82 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 81/82 [10:09<00:07,  7.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 81/82 [10:13<00:07,  7.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(12291, 1536)\n",
            "Shape of labels vector:(12291, 8)\n",
            "Test Feature Extraction:\n",
            "Beginning feature extraction for 3135 samples in 21 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 20/21 [02:26<00:07,  7.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 20/21 [02:29<00:07,  7.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(3135, 1536)\n",
            "Shape of labels vector:(3135, 8)\n",
            "Found 12291 images belonging to 8 classes.\n",
            "Found 3135 images belonging to 8 classes.\n",
            "Train Feature Extraction:\n",
            "Beginning feature extraction for 12291 samples in 82 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 81/82 [12:27<00:09,  9.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 81/82 [12:34<00:09,  9.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(12291, 2560)\n",
            "Shape of labels vector:(12291, 8)\n",
            "Test Feature Extraction:\n",
            "Beginning feature extraction for 3135 samples in 21 batches:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 20/21 [02:49<00:08,  8.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 20/21 [02:55<00:08,  8.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted!\n",
            "Shape of feature vector:(3135, 2560)\n",
            "Shape of labels vector:(3135, 8)\n"
          ]
        }
      ]
    }
  ]
}