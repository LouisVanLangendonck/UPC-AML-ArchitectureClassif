{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uIX-EmUHalpM",
        "ZrPzeqOyatIb",
        "_-spa0ZQa0_E",
        "mD2ZgfzvbT53",
        "pUES3dKXb-VD",
        "SiE1rfvYe7vF",
        "p15t1dzjfKIu",
        "oiJGjt1Mfpxe",
        "TTg72kxdgyV3",
        "yurTxzZWg1iy",
        "HjQKl5VXhUgm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LouisVanLangendonck/UPC-AML-ArchitectureClassif/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM models on extracted features\n",
        "\n",
        "This file contains three main sections:\n",
        " 1. SVM models on single kernels (linear, rbf, polynomial) and weighted sums and products \n",
        " 2. SVM models for concatenated vectors (a concatenated vector of 4 or 5 feature vectors is considered)\n",
        " 3. SVM models obtained using MKLpy package (used for direct sum of kernels, i.e. kernels constructed from different feature spaces and then added together)."
      ],
      "metadata": {
        "id": "uIX-EmUHalpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SVM models on single kernels, weighted sums, and products"
      ],
      "metadata": {
        "id": "ZrPzeqOyatIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and preparation"
      ],
      "metadata": {
        "id": "_-spa0ZQa0_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-cm40lwZa3e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/aml/models/extracted_features/inception_resnet_v2.npy\""
      ],
      "metadata": {
        "id": "lpyThFu2bGIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_extracted_feats(PATH):\n",
        "  \n",
        "  \"\"\"\n",
        "    loads extracted features via PATH\n",
        "  \"\"\"\n",
        "\n",
        "  np_load_old = np.load\n",
        "  # modify the default parameters of np.load\n",
        "  np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "  (train_features, train_labels), (test_features, test_labels) = np.load(PATH)\n",
        "\n",
        "  return train_features, train_labels, test_features, test_labels\n",
        "\n",
        "  def sets_for_SVM(size_train, size_test, train_features, train_labels, test_features, test_labels):\n",
        "\n",
        "  \"\"\"\n",
        "    Returns train, val,  test set with appropiate sizes\n",
        "\n",
        "  \"\"\"\n",
        "  tf = train_features\n",
        "  \n",
        "  tl = [0 for j in range(len(train_labels))]\n",
        "  for i in range(len(train_labels)):\n",
        "    for idx, val in enumerate(train_labels[i, ]):\n",
        "      if val == 1:\n",
        "        tl[i] = idx + 1\n",
        "\n",
        "  tl = np.array(tl)\n",
        "\n",
        "  test_f = test_features\n",
        "\n",
        "  test_l = [0 for j in range(len(test_labels))]\n",
        "  for i in range(len(test_labels)):\n",
        "    for idx, val in enumerate(test_labels[i, ]):\n",
        "      if val == 1:\n",
        "        test_l[i] = idx + 1\n",
        "\n",
        "\n",
        "  return tf, tl, test_f, test_l\n",
        "\n",
        "def separation_train_val(train_features, train_labels):\n",
        "  \"\"\"\n",
        "    There is an initla separation between train and validation. This validation \n",
        "    set is so it is possible to compare all models (NN, SVMs...)\n",
        "  \"\"\"\n",
        "\n",
        "  inp_train_features, inp_val_features, train_labels, val_labels = train_test_split(train_features, train_labels, stratify=train_labels, shuffle=True,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=1)\n",
        "  return inp_train_features, inp_val_features, train_labels, val_labels"
      ],
      "metadata": {
        "id": "xdE8XlX0bHkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.metrics import f1_score\n",
        "from time import time\n",
        "import time\n",
        "from matplotlib.colors import Normalize\n",
        "import os\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle "
      ],
      "metadata": {
        "id": "JP_AbrOSbOH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels, test_features, test_labels = load_extracted_feats(PATH)\n",
        "size_train = train_features.shape[0]\n",
        "size_test = test_features.shape[0]\n",
        "train_features, train_labels, test_features, test_labels = sets_for_SVM(size_train, size_test, train_features, train_labels, test_features, test_labels)\n",
        "train_features, val_features, train_labels, val_labels = separation_train_val(train_features, train_labels)\n",
        "with open('/content/drive/MyDrive/aml/models/extracted_features/class_encoding.pkl', 'rb') as f:\n",
        "  class_encoding = pickle.load(f)"
      ],
      "metadata": {
        "id": "zfwElKPkbc39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM models (single linear, gaussian rbf, polynomial)"
      ],
      "metadata": {
        "id": "mD2ZgfzvbT53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear kernel\n",
        "C = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
        "svm_val_score = np.zeros(len(C))\n",
        "for i, l in enumerate(C):\n",
        "    svm = LinearSVC(C=l, class_weight=\"balanced\", loss=\"hinge\")\n",
        "    svm.fit(train_features, train_labels)\n",
        "    val_pred = svm.predict(val_features)\n",
        "    svm_val_score[i] = f1_score(val_labels, val_pred, average='weighted')\n",
        "    print(\"C= \"+str(l) + \" done\")\n",
        "\n",
        "# show validation scores\n",
        "for i, l in enumerate(C):\n",
        "    print(f'\\t C={l}; validation_score (Weighted): {svm_val_score[i]}')\n",
        "print()\n",
        "    \n",
        "# select winner..\n",
        "best = np.argmax(svm_val_score)\n",
        "best_C = C[best]\n",
        "print(f'Best C in SVM is {best_C} with score {svm_val_score[best]}')\n"
      ],
      "metadata": {
        "id": "NNK0lNKRbZ1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = [0.01, 0.1, 1, 10,100]\n",
        "gamma = [1e-4, 1e-3, 1e-2, 1e-1]\n",
        "svm_val_score = np.zeros((len(C), len(gamma)))\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(gamma):\n",
        "      svm = SVC(C=l, gamma=m, kernel=\"rbf\", class_weight=\"balanced\")\n",
        "      svm.fit(train_features, train_labels)\n",
        "      val_pred = svm.predict(val_features)\n",
        "      svm_val_score[i,j] = f1_score(val_labels, val_pred, average='weighted')\n",
        "      print(\"C= \"+str(l) + \"gamma = \" + str(m) +\" done\")\n",
        "\n",
        "\n",
        "# show validation scores\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(gamma):\n",
        "    print(f'\\t C={l} gamma={m}; validation_score: {svm_val_score[i,j]}')\n",
        "print()\n",
        "    \n"
      ],
      "metadata": {
        "id": "0f07HK-Tbkvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = [0.01, 0.1, 1, 10]\n",
        "coef0 = [0, 1, 10]\n",
        "\n",
        "d = [2,3,4]\n",
        "svm_val_score_weighted = np.zeros((len(C), len(coef0), len(d)))\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(coef0):\n",
        "    for k, n in enumerate(d):\n",
        "      svm = SVC(C=l, coef0=m, degree=n, kernel=\"poly\", class_weight=\"balanced\")\n",
        "      svm.fit(train_features, train_labels)\n",
        "      val_pred = svm.predict(val_features)\n",
        "      svm_val_score_weighted[i,j,k] = f1_score(val_labels, val_pred, average='weighted')\n",
        "      print(\"C= \"+str(l) + \"c = \" + str(m) + \"degree\" + str(n) + \"done\")\n",
        "\n",
        "\n",
        "# show validation scores\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(coef0):\n",
        "    for k,n in enumerate(d):\n",
        "      print(f'\\t C={l} c={m} degree={n}; weighted_score: {svm_val_score_weighted[i,j,k]}')\n",
        "print()"
      ],
      "metadata": {
        "id": "NltYbMR0bq9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM models (weighted sum and product of kernels)"
      ],
      "metadata": {
        "id": "pUES3dKXb-VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear + RBF\n",
        "\n",
        "max_val_score = 0\n",
        "max_wei_score = 0\n",
        "param = [0,0,0]\n",
        "\n",
        "klinear =  np.dot(train_features, train_features.T)\n",
        "klinear_val = np.dot(val_features, train_features.T)\n",
        "\n",
        "gamma = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "\n",
        "for gi in gamma:\n",
        "  krbf = rbf_kernel(train_features, train_features, gamma=gi)\n",
        "  krbf_val = rbf_kernel(val_features, train_features, gamma=gi)\n",
        "  \n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "  print(\"NOW doing gamma= \" + str(gi))\n",
        "  b = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "  for bi in b:\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(\"NOW doing b= \" + str(bi))\n",
        "    k = bi*klinear + (1-bi)*krbf\n",
        "    kv = bi*klinear_val + (1-bi)*krbf_val\n",
        "\n",
        "    C = [1e-2, 1e-1, 1, 10]\n",
        "    svm_val_score = np.zeros(len(C))\n",
        "    for i, l in enumerate(C):\n",
        "        svm = SVC(kernel=\"precomputed\", C=l, class_weight=\"balanced\")\n",
        "        svm.fit(k, train_labels)\n",
        "        val_pred = svm.predict(kv)\n",
        "\n",
        "        if f1_score(val_pred, val_labels, average='weighted') >= max_wei_score:\n",
        "            max_val_score = f1_score(val_labels, val_pred, average='micro')\n",
        "            max_wei_score = f1_score(val_labels, val_pred, average='weighted')\n",
        "            \n",
        "            param[0] = gi\n",
        "            param[1] = bi\n",
        "            param[2] = l\n",
        "\n",
        "        print(\"C= \"+str(l) + \" done\")\n",
        "\n",
        "print(max_val_score)\n",
        "print(max_wei_score)\n",
        "print(param)\n"
      ],
      "metadata": {
        "id": "A6FlOPiicF4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial + RBF\n",
        "max_val_score = 0\n",
        "max_wei_score = 0\n",
        "param = [0,0,0,0,0]\n",
        "\n",
        "gamma = [0.001, 0.005, 0.01, 0.05,0.1]\n",
        "d  = [2,3,4]\n",
        "coef0 = [0,1]\n",
        "\n",
        "for gi in gamma:\n",
        "  krbf = rbf_kernel(train_features, train_features, gamma = gi)\n",
        "  krbf_val = rbf_kernel(val_features, train_features, gamma= gi)\n",
        "\n",
        "  for di in d:\n",
        "    for ci in coef0:\n",
        "      print(\"----------------------------------------------------------------------------------------------\")\n",
        "      print(\"PARAMETERS gamma = \" + str(gi) + \" d = \" + str(di) + \" coef0 = \" + str(ci))\n",
        "      kpoly =  polynomial_kernel(train_features, train_features, degree=di, coef0 = ci)\n",
        "      kpoly_val = polynomial_kernel(val_features, train_features, degree=di, coef0 =ci)\n",
        "      b = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "      for bi in b:\n",
        "        print(\"----------------------------------------------\")\n",
        "        print(\"NOW doing b= \" + str(bi))\n",
        "        k = bi*kpoly + (1-bi)*krbf\n",
        "        kv = bi*kpoly_val + (1-bi)*krbf_val\n",
        "\n",
        "        C = [1e-2, 1e-1, 1, 10]\n",
        "        svm_val_score = np.zeros(len(C))\n",
        "        for i, l in enumerate(C):\n",
        "            svm = SVC(kernel=\"precomputed\", C=l,class_weight=\"balanced\")\n",
        "            svm.fit(k, train_labels)\n",
        "            val_pred = svm.predict(kv)\n",
        "            \n",
        "\n",
        "            if f1_score(val_pred, val_labels, average='weighted') >= max_wei_score:\n",
        "              max_wei_score = f1_score(val_labels, val_pred, average='weighted')\n",
        "              max_val_score = f1_score(val_labels, val_pred, average='micro')\n",
        "\n",
        "              param[0] = gi\n",
        "              param[1] = di\n",
        "              param[2] = ci\n",
        "              param[3] = bi\n",
        "              param[4] = l\n",
        "\n",
        "        print(\"---------------------------------------------\")\n",
        "\n",
        "print(max_val_score)\n",
        "print(max_wei_score)\n",
        "print(param)"
      ],
      "metadata": {
        "id": "NwJKdyUicVeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear * RBF\n",
        "\n",
        "max_val_score = 0\n",
        "max_wei_score = 0\n",
        "param = [0,0]\n",
        "\n",
        "klinear =  np.dot(train_features, train_features.T)\n",
        "klinear_val = np.dot(val_features, train_features.T)\n",
        "\n",
        "gamma = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "\n",
        "for gi in gamma:\n",
        "  krbf = rbf_kernel(train_features, train_features, gamma=gi)\n",
        "  krbf_val = rbf_kernel(val_features, train_features, gamma=gi)\n",
        "  \n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "  print(\"NOW doing gamma= \" + str(gi))\n",
        "  k = np.multiply(klinear, krbf)\n",
        "  kv = np.multiply(klinear_val, krbf_val)\n",
        "  C = [1e-2, 1e-1, 1, 10]\n",
        "  svm_val_score = np.zeros(len(C))\n",
        "  for i, l in enumerate(C):\n",
        "      svm = SVC(kernel=\"precomputed\", C=l,class_weight=\"balanced\")\n",
        "      svm.fit(k, train_labels)\n",
        "      val_pred = svm.predict(kv)\n",
        "\n",
        "      if f1_score(val_pred, val_labels, average='weighted') >= max_wei_score:\n",
        "          max_val_score = f1_score(val_labels, val_pred, average='micro')\n",
        "          max_wei_score = f1_score(val_labels, val_pred, average='weighted')\n",
        "          \n",
        "          param[0] = gi\n",
        "          param[1] = l\n",
        "\n",
        "      print(\"C= \"+str(l) + \" done\")\n",
        "\n",
        "print(max_val_score)\n",
        "print(max_wei_score)\n",
        "print(param)"
      ],
      "metadata": {
        "id": "cI0kQicCcfqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RBF * Polynomial\n",
        "\n",
        "max_val_score = 0\n",
        "max_wei_score = 0\n",
        "param = [0,0,0,0]\n",
        "\n",
        "gamma = [0.001, 0.005, 0.01, 0.05,0.1]\n",
        "d  = [2,3,4]\n",
        "coef0 = [0,1]\n",
        "\n",
        "for gi in gamma:\n",
        "  krbf = rbf_kernel(train_features, train_features, gamma = gi)\n",
        "  krbf_val = rbf_kernel(val_features, train_features, gamma= gi)\n",
        "\n",
        "  for di in d:\n",
        "    for ci in coef0:\n",
        "      print(\"----------------------------------------------------------------------------------------------\")\n",
        "      print(\"PARAMETERS gamma = \" + str(gi) + \" d = \" + str(di) + \" coef0 = \" + str(ci))\n",
        "      kpoly =  polynomial_kernel(train_features, train_features, degree=di, coef0 = ci)\n",
        "      kpoly_val = polynomial_kernel(val_features, train_features, degree=di, coef0 =ci)\n",
        "      \n",
        "      k = np.multiply(krbf, kpoly)\n",
        "      kv = np.multiply(krbf_val, kpoly_val)\n",
        "\n",
        "      C = [1e-2, 1e-1, 1, 10]\n",
        "      svm_val_score = np.zeros(len(C))\n",
        "      for i, l in enumerate(C):\n",
        "\n",
        "          svm = SVC(kernel=\"precomputed\", C=l, class_weight=\"balanced\")\n",
        "          svm.fit(k, train_labels)\n",
        "          val_pred = svm.predict(kv)\n",
        "         \n",
        "          print(\"C= \"+str(l) + \" done\")\n",
        "\n",
        "          if f1_score(val_pred, val_labels, average='weighted') >= max_wei_score:\n",
        "            max_wei_score = f1_score(val_labels, val_pred, average='weighted')\n",
        "            max_val_score = f1_score(val_labels, val_pred, average='micro')\n",
        "\n",
        "            param[0] = gi\n",
        "            param[1] = di\n",
        "            param[2] = ci\n",
        "            param[3] = l\n",
        "\n",
        "      print(\"---------------------------------------------\")\n",
        "print(max_val_score)\n",
        "print(max_wei_score)\n",
        "print(param)"
      ],
      "metadata": {
        "id": "8hHKtQa2eq6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RBF * RBF\n",
        "max_val_score = 0\n",
        "max_wei_score = 0\n",
        "param = [0,0]\n",
        "\n",
        "\n",
        "\n",
        "gamma = [0.001, 0.005, 0.01, 0.05 ,0.1]\n",
        "for gi in gamma:\n",
        "  krbf1 = rbf_kernel(train_features, train_features, gamma=gi)\n",
        "  krbf1_val = rbf_kernel(val_features, train_features, gamma=gi)\n",
        "  krbf2 = rbf_kernel(train_features, train_features, gamma=gi)\n",
        "  krbf2_val = rbf_kernel(val_features, train_features, gamma=gi)\n",
        "  \n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "  print(\"NOW doing gamma= \" + str(gi))\n",
        "  k = np.multiply(krbf1, krbf2)\n",
        "  kv = np.multiply(krbf1_val, krbf2_val)\n",
        "  C = [1e-2, 1e-1, 1, 10]\n",
        "  svm_val_score = np.zeros(len(C))\n",
        "  for i, l in enumerate(C):\n",
        "      svm = SVC(kernel=\"precomputed\", C=l,class_weight=\"balanced\")\n",
        "      svm.fit(k, train_labels)\n",
        "      val_pred = svm.predict(kv)\n",
        "\n",
        "      if f1_score(val_pred, val_labels, average='weighted') >= max_wei_score:\n",
        "          max_val_score = f1_score(val_labels, val_pred, average='micro')\n",
        "          max_wei_score = f1_score(val_labels, val_pred, average='weighted')\n",
        "          \n",
        "          param[0] = gi\n",
        "          param[1] = l\n",
        "          \n",
        "      print(\"C= \"+str(l) + \" done\")\n",
        "\n",
        "print(max_val_score)\n",
        "print(max_wei_score)\n",
        "print(param)"
      ],
      "metadata": {
        "id": "gDOVmn8Pez3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. SVM models for concatenated vectors"
      ],
      "metadata": {
        "id": "SiE1rfvYe7vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and preparation"
      ],
      "metadata": {
        "id": "p15t1dzjfKIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_extracted_feats(PATH):\n",
        "  \n",
        "  \"\"\"\n",
        "    loads extracted features via PATH\n",
        "  \"\"\"\n",
        "\n",
        "  np_load_old = np.load\n",
        "  # modify the default parameters of np.load\n",
        "  np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "  (train_features, train_labels), (test_features, test_labels) = np.load(PATH)\n",
        "\n",
        "  return train_features, train_labels, test_features, test_labels\n",
        "def sets_for_SVM(size_train, size_test, train_features, train_labels, test_features, test_labels):\n",
        "\n",
        "  \"\"\"\n",
        "    Returns train, val,  test set with appropiate sizes\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  tf = train_features\n",
        "  \n",
        "  #tf = np.resize(train_features, (size_train, 2048)) \n",
        "  tl = [0 for j in range(len(train_labels))]\n",
        "  for i in range(len(train_labels)):\n",
        "    for idx, val in enumerate(train_labels[i, ]):\n",
        "      if val == 1:\n",
        "        tl[i] = idx + 1\n",
        "\n",
        "  tl = np.array(tl)\n",
        "\n",
        "\n",
        "  #test_f = np.resize(test_features, (size_test, 7*7*512))\n",
        "  test_f = test_features\n",
        "\n",
        "  test_l = [0 for j in range(len(test_labels))]\n",
        "  for i in range(len(test_labels)):\n",
        "    for idx, val in enumerate(test_labels[i, ]):\n",
        "      if val == 1:\n",
        "        test_l[i] = idx + 1\n",
        "\n",
        "\n",
        "  return tf, tl, test_f, test_l\n",
        "\n",
        "\n",
        "def separation_train_val(train_features, train_labels):\n",
        "\"\"\"\n",
        "  There is an initla separation between train and validation. This validation \n",
        "  set is so it is possible to compare all models (NN, SVMs...)\n",
        "\"\"\"\n",
        "\n",
        "  inp_train_features, inp_val_features, train_labels, val_labels = train_test_split(train_features, train_labels, stratify=train_labels, shuffle=True,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=1)\n",
        "  return inp_train_features, inp_val_features, train_labels, val_labels\n"
      ],
      "metadata": {
        "id": "tNFkfSexfRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.metrics import f1_score\n",
        "from time import time\n",
        "import os\n",
        "import time\n",
        "from matplotlib.colors import Normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle "
      ],
      "metadata": {
        "id": "OhuE8VYJffpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat_path_base = '/content/drive/MyDrive/aml/models/extracted_features/'\n",
        "concat_path_list = ['inception_resnet_v2.npy', 'vgg19.npy', 'xception.npy', 'SIFT.npy', 'efficientnetb7.npy']\n",
        "train_features = []\n",
        "test_features = []\n",
        "\n",
        "np_load_old = np.load\n",
        "  # modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "for idx, feature_file in enumerate(concat_path_list):\n",
        "    feature_path = os.path.join(concat_path_base, feature_file)\n",
        "    (train_features_to_concat, train_labels), (test_features_to_concat, test_labels) = np.load(feature_path)\n",
        "    if idx == 0:\n",
        "      train_features = train_features_to_concat\n",
        "      test_features = test_features_to_concat\n",
        "    else:\n",
        "      train_features=np.append(train_features, train_features_to_concat, 1)\n",
        "      test_features=np.append(test_features, test_features_to_concat, 1)"
      ],
      "metadata": {
        "id": "_AsosxnKfhtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_train = train_features.shape[0]\n",
        "size_test = test_features.shape[0]\n",
        "train_features, train_labels, test_features, test_labels = sets_for_SVM(size_train, size_test, train_features, train_labels, test_features, test_labels)\n",
        "train_features, val_features, train_labels, val_labels = separation_train_val(train_features, train_labels)\n",
        "with open('/content/drive/MyDrive/aml/models/extracted_features/class_encoding.pkl', 'rb') as f:\n",
        "  class_encoding = pickle.load(f)"
      ],
      "metadata": {
        "id": "Sney9IVLfkMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear, Gaussian RBF,Polynomial (for concatenated vector)"
      ],
      "metadata": {
        "id": "oiJGjt1Mfpxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Kernel\n",
        "\n",
        "C = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
        "svm_val_score = np.zeros(len(C))\n",
        "for i, l in enumerate(C):\n",
        "    svm = LinearSVC(C=l, class_weight=\"balanced\")\n",
        "    svm.fit(train_features, train_labels)\n",
        "    val_pred = svm.predict(val_features)\n",
        "    svm_val_score[i] = f1_score(val_labels, val_pred, average='weighted')\n",
        "    print(\"C= \"+str(l) + \" done\")\n",
        "\n",
        "# show validation scores\n",
        "for i, l in enumerate(C):\n",
        "    print(f'\\t C={l}; validation_score (weighted): {svm_val_score[i]}')\n",
        "print()\n",
        "    \n",
        "# select winner..\n",
        "best = np.argmax(svm_val_score)\n",
        "best_C = C[best]\n",
        "print(f'Best C in SVM is {best_C} with score {svm_val_score[best]}')"
      ],
      "metadata": {
        "id": "eMjVHMMLfurn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RBF Kernel\n",
        "C = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
        "gamma = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "svm_val_score = np.zeros((len(C), len(gamma)))\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(gamma):\n",
        "      svm = SVC(C=l, gamma=m, kernel=\"rbf\", class_weight=\"balanced\")\n",
        "      svm.fit(train_features, train_labels)\n",
        "      val_pred = svm.predict(val_features)\n",
        "      svm_val_score[i,j] = f1_score(val_labels, val_pred, average='weighted')\n",
        "      print(\"C= \"+str(l) + \"gamma = \" + str(m) +\" done\")\n",
        "\n",
        "\n",
        "# show validation scores\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(gamma):\n",
        "    print(f'\\t C={l} gamma={m}; validation_score: {svm_val_score[i,j]}')\n",
        "print()"
      ],
      "metadata": {
        "id": "FxCfdArBgoZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial\n",
        "\n",
        "C = [1, 10, 100]\n",
        "coef0 = [0, 1]\n",
        "d = [2,3,4]\n",
        "svm_val_score = np.zeros((len(C), len(coef0), len(d)))\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(coef0):\n",
        "    for k, n in enumerate(d):\n",
        "      svm = SVC(C=l, coef0=m, degree=n, kernel=\"poly\")\n",
        "      svm.fit(train_features, train_labels)\n",
        "      val_pred = svm.predict(val_features)\n",
        "      svm_val_score[i,j,k] = f1_score(val_labels, val_pred, average='weighted')\n",
        "      print(\"C= \"+str(l) + \"c = \" + str(m) + \"degree\" + str(n) + \"done\")\n",
        "\n",
        "\n",
        "# show validation scores\n",
        "for i, l in enumerate(C):\n",
        "  for j, m in enumerate(coef0):\n",
        "    for k,n in enumerate(d):\n",
        "      print(f'\\t C={l} c={m} degree={n}; validation_score: {svm_val_score[i,j,k]}')\n",
        "print()"
      ],
      "metadata": {
        "id": "DocmEROsgur0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.SVM with a kernel obtained via MKL"
      ],
      "metadata": {
        "id": "TTg72kxdgyV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and preparation"
      ],
      "metadata": {
        "id": "yurTxzZWg1iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MKLpy\n",
        "import MKLpy\n"
      ],
      "metadata": {
        "id": "4_m_Gnfyg48Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sets_for_SVM(size_train, size_test, train_features, train_labels, test_features, test_labels):\n",
        "\n",
        "  \"\"\"\n",
        "    Returns train, val,  test set with appropiate sizes\n",
        "\n",
        "  \"\"\"\n",
        "  tf = train_features\n",
        "  \n",
        "  #tf = np.resize(train_features, (size_train, 2048)) \n",
        "  tl = [0 for j in range(len(train_labels))]\n",
        "  for i in range(len(train_labels)):\n",
        "    for idx, val in enumerate(train_labels[i, ]):\n",
        "      if val == 1:\n",
        "        tl[i] = idx + 1\n",
        "\n",
        "  tl = np.array(tl)\n",
        "\n",
        "\n",
        "  #test_f = np.resize(test_features, (size_test, 7*7*512))\n",
        "  test_f = test_features\n",
        "\n",
        "  test_l = [0 for j in range(len(test_labels))]\n",
        "  for i in range(len(test_labels)):\n",
        "    for idx, val in enumerate(test_labels[i, ]):\n",
        "      if val == 1:\n",
        "        test_l[i] = idx + 1\n",
        "\n",
        "\n",
        "  return tf, tl, test_f, test_l\n",
        "\n",
        "def separation_train_val(train_features, train_labels):\n",
        "  \"\"\"\n",
        "    There is an initla separation between train and validation. This validation \n",
        "    set is so it is possible to compare all models (NN, SVMs...)\n",
        "  \"\"\"\n",
        "\n",
        "  inp_train_features, inp_val_features, train_labels, val_labels = train_test_split(train_features, train_labels, stratify=train_labels, shuffle=True,\n",
        "                                                    test_size=0.20, \n",
        "                                                    random_state=1)\n",
        "  return inp_train_features, inp_val_features, train_labels, val_labels"
      ],
      "metadata": {
        "id": "wPQVdGcLg7aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from sklearn.metrics import f1_score\n",
        "from time import time\n",
        "import os\n",
        "import time\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle "
      ],
      "metadata": {
        "id": "lCAAnxuug_KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summ_path = '/content/drive/MyDrive/aml/models/extracted_features/'\n",
        "summ_duo = ['SIFT.npy', 'vgg19.npy', 'xception.npy', 'inception_resnet_v2.npy']\n",
        "\n",
        "\n",
        "np_load_old = np.load\n",
        "  # modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "feature_path = os.path.join(summ_path, summ_duo[0])\n",
        "(train_features_1, train_labels_1), (test_features_1, test_labels_1) = np.load(feature_path)\n",
        "feature_path = os.path.join(summ_path, summ_duo[1])\n",
        "(train_features_2, train_labels_2), (test_features_2, test_labels_2) = np.load(feature_path)\n",
        "feature_path = os.path.join(summ_path, summ_duo[2])\n",
        "(train_features_3, train_labels_3), (test_features_3, test_labels_3) = np.load(feature_path)\n",
        "feature_path = os.path.join(summ_path, summ_duo[3])\n",
        "(train_features_4, train_labels_4), (test_features_4, test_labels_4) = np.load(feature_path)\n",
        "\n",
        "\n",
        "size_train_1 = train_features_1.shape[0]\n",
        "size_test_1 = test_features_1.shape[0]\n",
        "train_features_1, train_labels_1, test_features_1, test_labels_1 = sets_for_SVM(size_train_1, size_test_1, train_features_1, train_labels_1, test_features_1, test_labels_1)\n",
        "\n",
        "size_train_2 = train_features_2.shape[0]\n",
        "size_test_2 = test_features_2.shape[0]\n",
        "train_features_2, train_labels_2, test_features_2, test_labels_2 = sets_for_SVM(size_train_2, size_test_2, train_features_2, train_labels_2, test_features_2, test_labels_2)\n",
        "\n",
        "size_train_3 = train_features_3.shape[0]\n",
        "size_test_3 = test_features_3.shape[0]\n",
        "train_features_3, train_labels_3, test_features_3, test_labels_3 = sets_for_SVM(size_train_3, size_test_3, train_features_3, train_labels_3, test_features_3, test_labels_3)\n",
        "\n",
        "size_train_4 = train_features_4.shape[0]\n",
        "size_test_4 = test_features_4.shape[0]\n",
        "train_features_4, train_labels_4, test_features_4, test_labels_4 = sets_for_SVM(size_train_4, size_test_4, train_features_4, train_labels_4, test_features_4, test_labels_4)\n",
        "\n",
        "with open('/content/drive/MyDrive/aml/models/extracted_features/class_encoding.pkl', 'rb') as f:\n",
        "  class_encoding = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "gtSsqFNdg_yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing the direct sum kernel and testing it on different algorithms"
      ],
      "metadata": {
        "id": "HjQKl5VXhUgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from MKLpy.preprocessing import kernel_normalization\n",
        "from MKLpy.model_selection import train_test_split\n",
        "\n",
        "K1 = rbf_kernel(train_features_1, train_features_1, gamma = 0.01)\n",
        "K2 = rbf_kernel(train_features_2, train_features_2, gamma = 0.01)\n",
        "K3 = rbf_kernel(train_features_2, train_features_2, gamma = 0.01)\n",
        "K4 = rbf_kernel(train_features_2, train_features_2, gamma = 0.01)\n",
        "\n",
        "KL = [K1,K2, K3, K4]\n"
      ],
      "metadata": {
        "id": "brj51LRphFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KLtr,KLte,Ytr,Yte = train_test_split(KL, train_labels_1, test_size=.2, random_state=1)"
      ],
      "metadata": {
        "id": "e1KQn83ghHoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AverageMKL\n",
        "\n",
        "from MKLpy.algorithms import AverageMKL\n",
        "from sklearn.metrics import f1_score\n",
        "C = [1e-2, 1e-1, 1, 10]\n",
        "mkl_svm_val_score = np.zeros(len(C))\n",
        "for i, l in enumerate(C):\n",
        "    mkl = AverageMKL(SVC(C=l))\n",
        "    mkl = mkl.fit(KLtr, Ytr)\n",
        "    y_pred = mkl.predict(KLte)\n",
        "    mkl_svm_val_score[i] = f1_score(Yte, y_pred, average='weighted')\n",
        "    print(\"C = \" +str(l) + \"weighted score\" + str(f1_score(Yte, y_pred, average='weighted')) )\n",
        "    print(\"C= \"+str(l) + \" done\")"
      ],
      "metadata": {
        "id": "YlWLWPn8hJe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from MKLpy.algorithms import EasyMKL, KOMD\t\n",
        "\n",
        "# EASY MKL\n",
        "\n",
        "from MKLpy.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "svm = SVC(C=10)\n",
        "clf = EasyMKL(lam=0, multiclass_strategy='ova', learner=svm).fit(KLtr,Ytr)\n",
        "from MKLpy.multiclass import OneVsRestMKLClassifier, OneVsOneMKLClassifier\n",
        "print ('done')\n",
        "print ('the combination weights are:')\n",
        "for sol in clf.solution:\n",
        "\tprint ('(%d vs all): ' % sol, clf.solution[sol].weights)\n",
        "\t\n",
        "#evaluate the solution\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import numpy as np\n",
        "y_pred = clf.predict(KLte)\t\t\t\t\t#predictions\n",
        "y_score = clf.decision_function(KLte)\t\t#rank\n",
        "accuracy = accuracy_score(Yte, y_pred)\n",
        "print ('Accuracy score: %.3f' % (accuracy))\n"
      ],
      "metadata": {
        "id": "DonUmF-HhMuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "easy_micro = f1_score(Yte, y_pred, average='micro')\n",
        "easy_weighted = f1_score(Yte, y_pred, average='weighted')\n",
        "print(easy_micro)\n",
        "print(easy_weighted)"
      ],
      "metadata": {
        "id": "H9yLEV5zhSAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}